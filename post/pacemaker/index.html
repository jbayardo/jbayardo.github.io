<!doctype html><html xmlns=http://www.w3.org/1999/xhtml lang=en><head><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><title>Pacemaking - a probabilistic load balancing trick | Julian through the Lens</title>
<meta property='og:title' content='Pacemaking - a probabilistic load balancing trick - Julian through the Lens'><meta property='og:description' content='The problem Imagine there&rsquo;s some number of servers $N$ in a fully-connected P2P network. These servers can copy files from each other. In this imaginary system, we have a single machine that every $\delta$ minutes produces a file that all other machines need to get a hold of (say, for example, a database checkpoint), and this file happens to be relatively large, let&rsquo;s say > 200GB for the sake of argument.'><meta property='og:url' content='https://julian.bayardo.info/post/pacemaker/'><meta property='og:site_name' content='Julian through the Lens'><meta property='og:type' content='article'><meta property='og:image' content='https://www.gravatar.com/avatar/fdbc036a46f3c0903c7e39459db599c0?s=256'><meta property='article:section' content='Post'><meta property='article:tag' content='probability'><meta property='article:tag' content='systems'><meta property='article:tag' content='load-balancing'><meta property='article:published_time' content='2024-08-18T20:53:00-07:00'><meta property='article:modified_time' content='2024-08-18T20:53:00-07:00'><meta name=twitter:card content='summary'><meta name=twitter:site content='@BayardoJulian'><meta name=twitter:creator content='@BayardoJulian'><link rel=alternate type=application/rss+xml title="Julian through the Lens" href=/post/index.xml><link rel=stylesheet href=/css/style.min.572ee845ad8da3c5d2a0a9e157da4b896325fafa6ff5b9b18ae122044000e00e.css><link rel=apple-touch-icon sizes=180x180 href=/apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/favicon-16x16.png><link rel=manifest href=/site.webmanifest><link rel=mask-icon href=/safari-pinned-tab.svg color=#5bbad5><link rel=canonical href=https://julian.bayardo.info/post/pacemaker/><meta name=msapplication-TileColor content="#da532c"><meta name=theme-color content="#ffffff"><script type=text/javascript src=https://julian.bayardo.info/js/vendor/plotly/plotly.min.js></script><script type=text/javascript src=https://julian.bayardo.info/js/vendor/mathjs/math.js></script></head><body><header class=section><div class=container><nav id=nav-main class=nav><div id=nav-name class=nav-left><a id=nav-anchor class=nav-item href=https://julian.bayardo.info/><h1 id=nav-heading class="title is-4">Julian through the Lens</h1></a></div><div class=nav-right><nav id=nav-items class="nav-item level is-mobile"><a class=level-item aria-label=github href=https://github.com/jbayardo target=_blank rel=noopener><span class=icon><i><svg viewbox="0 0 24 24" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" aria-hidden="true"><path d="M9 19c-5 1.5-5-2.5-7-3m14 6v-3.87a3.37 3.37.0 00-.94-2.61c3.14-.35 6.44-1.54 6.44-7A5.44 5.44.0 0020 4.77 5.07 5.07.0 0019.91 1S18.73.65 16 2.48a13.38 13.38.0 00-7 0C6.27.65 5.09 1 5.09 1A5.07 5.07.0 005 4.77 5.44 5.44.0 003.5 8.55c0 5.42 3.3 6.61 6.44 7A3.37 3.37.0 009 18.13V22"/></svg>
</i></span></a><a class=level-item aria-label=email href=mailto:julian@bayardo.info target=_blank rel=noopener><span class=icon><i><svg viewbox="0 0 24 24" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" aria-hidden="true"><path d="M4 4h16c1.1.0 2 .9 2 2v12c0 1.1-.9 2-2 2H4c-1.1.0-2-.9-2-2V6c0-1.1.9-2 2-2z"/><polyline points="22,6 12,13 2,6"/></svg>
</i></span></a><a class=level-item aria-label=linkedin href=https://linkedin.com/in/jbayardo target=_blank rel=noopener><span class=icon><i><svg viewbox="0 0 24 24" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" aria-hidden="true"><path stroke-width="1.8" d="m5.839218 4.101561c0 1.211972-.974141 2.194011-2.176459 2.194011S1.4863 5.313533 1.4863 4.101561c0-1.211094.974141-2.194011 2.176459-2.194011s2.176459.982917 2.176459 2.194011zm.017552 3.94922H1.468748v14.04167H5.85677V8.050781zm7.005038.0H8.501869v14.04167h4.360816v-7.370999c0-4.098413 5.291077-4.433657 5.291077.0v7.370999h4.377491v-8.89101c0-6.915523-7.829986-6.66365-9.669445-3.259423V8.050781z"/></svg></i></span></a></nav></div></nav><nav class=nav><div class=nav-left><a class=nav-item href=/search><h2 class="title is-5">Search</h2></a><a class=nav-item href=/series><h2 class="title is-5">Series</h2></a><a class=nav-item href=/tags><h2 class="title is-5">Tags</h2></a></div><div class=nav-right><a class=nav-item href=/about><h2 class="title is-5">About me</h2></a><a class=nav-item href=/disclaimer><h2 class="title is-5">Disclaimer</h2></a></div></nav></div><script src=/js/navicon-shift.min.c7620c1a856fc9aead68a6e5fa7d6a739ac88b0ef5eb6834fb6dacf7dc82adc3.js integrity="sha256-x2IMGoVvya6taKbl+n1qc5rIiw7162g0+22s99yCrcM=" crossorigin=anonymous async defer></script></header><section class="section article-section"><article class=article data-pagefind-body><div class=article-container><div class=container><div class="subtitle tags is-6 is-pulled-right"><a class="tag is-6 is-link" href=/tags/probability data-pagefind-filter=tag><svg viewbox="0 0 24 24" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" aria-hidden="true"><path d="M20.59 13.41l-7.17 7.17a2 2 0 01-2.83.0L2 12V2H12l8.59 8.59a2 2 0 010 2.82z"/><line x1="7" y1="7" x2="7" y2="7"/></svg>
probability</a>
<a class="tag is-6 is-link" href=/tags/systems data-pagefind-filter=tag><svg viewbox="0 0 24 24" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" aria-hidden="true"><path d="M20.59 13.41l-7.17 7.17a2 2 0 01-2.83.0L2 12V2H12l8.59 8.59a2 2 0 010 2.82z"/><line x1="7" y1="7" x2="7" y2="7"/></svg>
systems</a>
<a class="tag is-6 is-link" href=/tags/load-balancing data-pagefind-filter=tag><svg viewbox="0 0 24 24" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" aria-hidden="true"><path d="M20.59 13.41l-7.17 7.17a2 2 0 01-2.83.0L2 12V2H12l8.59 8.59a2 2 0 010 2.82z"/><line x1="7" y1="7" x2="7" y2="7"/></svg>
load-balancing</a></div><h2 class="subtitle is-6" data-pagefind-meta=date>August 18, 2024</h2><h1 class=title data-pagefind-meta=title>Pacemaking - a probabilistic load balancing trick</h1><div class="content article"><nav class="table-of-contents boxed"><h2 class=title>Table of Contents</h2><nav id=TableOfContents><ol><li><a href=#the-problem>The problem</a></li><li><a href=#the-challenge>The challenge</a></li><li><a href=#pacemaking>Pacemaking</a></li><li><a href=#distributing-downloads-over-time>Distributing downloads over time</a></li><li><a href=#allocating-machines-to-time-buckets>Allocating machines to time buckets</a></li><li><a href=#at-last-the-algorithm>At last, the algorithm</a></li><li><a href=#what-was-thrown-under-the-rug>What was thrown under the rug</a></li><li><a href=#conclusion>Conclusion</a></li></ol></nav></nav><h1 id=the-problem>The problem
<a href=#the-problem style=float:right;border-bottom:0><svg width="16" height="16" viewBox="0 0 8 8"><path d="M5.88.03c-.18.01-.36.03-.53.09-.27.1-.53.25-.75.47a.5.5.0 10.69.69c.11-.11.24-.17.38-.22.35-.12.78-.07 1.06.22.39.39.39 1.04.0 1.44l-1.5 1.5c-.44.44-.8.48-1.06.47-.26-.01-.41-.13-.41-.13a.5.5.0 10-.5.88s.34.22.84.25 1.2-.16 1.81-.78l1.5-1.5c.78-.78.78-2.04.0-2.81-.28-.28-.61-.45-.97-.53-.18-.04-.38-.04-.56-.03zm-2 2.31c-.5-.02-1.19.15-1.78.75L.6 4.59c-.78.78-.78 2.04.0 2.81.56.56 1.36.72 2.06.47.27-.1.53-.25.75-.47a.5.5.0 10-.69-.69c-.11.11-.24.17-.38.22-.35.12-.78.07-1.06-.22-.39-.39-.39-1.04.0-1.44l1.5-1.5c.4-.4.75-.45 1.03-.44.28.01.47.09.47.09a.5.5.0 10.44-.88s-.34-.2-.84-.22z"/></svg></a></h1><p>Imagine there&rsquo;s some number of servers $N$ in a fully-connected P2P network. These servers can copy files from each other. In this imaginary system, we have a single machine that every $\delta$ minutes produces a file that all other machines need to get a hold of (say, for example, a database checkpoint), and this file happens to be relatively large, let&rsquo;s say > 200GB for the sake of argument.</p><p>Consumers of the file would need to learn about the file&rsquo;s existence, and then download it. We could imagine a push notification system, or some sort of polling system, but for the sake of simplicity let&rsquo;s assume the producer stores a record of the existence of this file in a shared location. Therefore, let&rsquo;s say every $1 \text{m}$, each server will check this shared location to see if there&rsquo;s a new file to download. If there is, it will download it from any of the machines that already have it. If there isn&rsquo;t, it will do nothing.</p><h1 id=the-challenge>The challenge
<a href=#the-challenge style=float:right;border-bottom:0><svg width="16" height="16" viewBox="0 0 8 8"><path d="M5.88.03c-.18.01-.36.03-.53.09-.27.1-.53.25-.75.47a.5.5.0 10.69.69c.11-.11.24-.17.38-.22.35-.12.78-.07 1.06.22.39.39.39 1.04.0 1.44l-1.5 1.5c-.44.44-.8.48-1.06.47-.26-.01-.41-.13-.41-.13a.5.5.0 10-.5.88s.34.22.84.25 1.2-.16 1.81-.78l1.5-1.5c.78-.78.78-2.04.0-2.81-.28-.28-.61-.45-.97-.53-.18-.04-.38-.04-.56-.03zm-2 2.31c-.5-.02-1.19.15-1.78.75L.6 4.59c-.78.78-.78 2.04.0 2.81.56.56 1.36.72 2.06.47.27-.1.53-.25.75-.47a.5.5.0 10-.69-.69c-.11.11-.24.17-.38.22-.35.12-.78.07-1.06-.22-.39-.39-.39-1.04.0-1.44l1.5-1.5c.4-.4.75-.45 1.03-.44.28.01.47.09.47.09a.5.5.0 10.44-.88s-.34-.2-.84-.22z"/></svg></a></h1><p>This kind of consumption pattern is prone to denial of service attacks on the producer of the file: if the producer is the only machine that has the file, then many machines are likely to try to download the file from the producer at the same time, which could easily overwhelm the producer. This is a classic example of a &ldquo;hot spot&rdquo; in a distributed system.</p><p>One thing to notice about this system is that it will almost always wind up stressing the producer:</p><ol><li>Machines are typically (due to deployment cycles) started up roughly at the same time, which means they will tend to come against the producer in groups.</li><li>Subset of machines may be enough to overwhelm the producer (and it practice it often is). Once the producer is overwhelmed and slow, more machines will pile on, making the problem worse. This is a positive feedback loop that can easily lead to a denial of service attack.</li><li>An alternative here is something in the form of a &ldquo;retry storm&rdquo;: the producer is slow, triggers timeouts (or straight up failures), and the machines retry, making the problem worse.</li></ol><p>This is not a happy situation to have if you want to sleep soundly at night (ask me how I know!). So, what can we do about it?</p><h1 id=pacemaking>Pacemaking
<a href=#pacemaking style=float:right;border-bottom:0><svg width="16" height="16" viewBox="0 0 8 8"><path d="M5.88.03c-.18.01-.36.03-.53.09-.27.1-.53.25-.75.47a.5.5.0 10.69.69c.11-.11.24-.17.38-.22.35-.12.78-.07 1.06.22.39.39.39 1.04.0 1.44l-1.5 1.5c-.44.44-.8.48-1.06.47-.26-.01-.41-.13-.41-.13a.5.5.0 10-.5.88s.34.22.84.25 1.2-.16 1.81-.78l1.5-1.5c.78-.78.78-2.04.0-2.81-.28-.28-.61-.45-.97-.53-.18-.04-.38-.04-.56-.03zm-2 2.31c-.5-.02-1.19.15-1.78.75L.6 4.59c-.78.78-.78 2.04.0 2.81.56.56 1.36.72 2.06.47.27-.1.53-.25.75-.47a.5.5.0 10-.69-.69c-.11.11-.24.17-.38.22-.35.12-.78.07-1.06-.22-.39-.39-.39-1.04.0-1.44l1.5-1.5c.4-.4.75-.45 1.03-.44.28.01.47.09.47.09a.5.5.0 10.44-.88s-.34-.2-.84-.22z"/></svg></a></h1><p>Load balancing consists of distributing the load across a set of servers. Typically, we have a bunch of servers that can be substituted for one another in replying for a request, and they are all online at the same time so we can use some heuristic to determine which server to reach out to.</p><p>In this case, we have a single server that is the bottleneck, and we want to distribute the load across all servers, so the load balancing is over <em>time</em>, rather than over <em>existing servers</em>.</p><p>There&rsquo;s plenty of ways to do this, for example, the producer could proactively send the file to future consumers before advertising it (guaranteeing there&rsquo;s a pool of machines in existence by the time it&rsquo;s advertised), or it could advertise the file only to a subset of machines, and then have those machines advertise the file to other machines, etc.</p><p>The issue with most of these ideas is that they are relatively costly in terms of implementation (for example, we now need to implement this thing which allows us to proactively send files!), and we all know no one likes large PRs with high maintenance cost. We already have a content distribution network in place, and a place to advertise the file&rsquo;s existence, so can we make do?</p><p>The key idea behind pacemaking is this: we can manipulate the chance of any given machine downloading a file in any given time period in order to ensure the load on the producer dissipates over time by adding more replicas in a controlled fashion. Let&rsquo;s dig in.</p><h1 id=distributing-downloads-over-time>Distributing downloads over time
<a href=#distributing-downloads-over-time style=float:right;border-bottom:0><svg width="16" height="16" viewBox="0 0 8 8"><path d="M5.88.03c-.18.01-.36.03-.53.09-.27.1-.53.25-.75.47a.5.5.0 10.69.69c.11-.11.24-.17.38-.22.35-.12.78-.07 1.06.22.39.39.39 1.04.0 1.44l-1.5 1.5c-.44.44-.8.48-1.06.47-.26-.01-.41-.13-.41-.13a.5.5.0 10-.5.88s.34.22.84.25 1.2-.16 1.81-.78l1.5-1.5c.78-.78.78-2.04.0-2.81-.28-.28-.61-.45-.97-.53-.18-.04-.38-.04-.56-.03zm-2 2.31c-.5-.02-1.19.15-1.78.75L.6 4.59c-.78.78-.78 2.04.0 2.81.56.56 1.36.72 2.06.47.27-.1.53-.25.75-.47a.5.5.0 10-.69-.69c-.11.11-.24.17-.38.22-.35.12-.78.07-1.06-.22-.39-.39-.39-1.04.0-1.44l1.5-1.5c.4-.4.75-.45 1.03-.44.28.01.47.09.47.09a.5.5.0 10.44-.88s-.34-.2-.84-.22z"/></svg></a></h1><p>We have $N$ servers that we want to distribute the file to over time. One way to distribute the load over time is to arange downloads in a way that machines downloading the file at any given time are roughly the same numbers as the machines that already have the file. To do so, imagine we split the time into buckets:</p><ul><li>At $T = 0$, we have exactly 1 machine (the producer) who has the file, so we want at most 1 machine to download the file.</li><li>At $T = 1$ -and assuming everything went well- we have 2 machines who have the file, so we want at most 2 machines to download the file.</li><li>At $T = 2$ -and assuming everything went well-, we have 4 machines who have the file, so we want at most 4 machines to download the file.</li></ul><p>So essentially, at the beginning of the $T$-th bucket of time, we want at most $2^T$ machines to download the file, as $2^{\text{max}(T-1, 1)}$ machines already have downloaded the file by that point in time. At the end of the $T$-th bucket of time, $2^{T+1}$ machines will have downloaded the file. Since we have $N$ machines that need to download the file, we need $\lceil \log_2(N) \rceil$ buckets over time to ensure all machines have downloaded the file; and the last bucket will have $N - 2^{\lfloor \log_2(N) \rfloor}$ machines downloading the file.</p><p>A plot of that looks something like this:</p><div id=distribution-plot></div><p>A few things are worthy of notice here:</p><ol><li>We do not actually have a way to distribute downloads over time such that they fit this distribution.</li><li>We are assuming all file downloads are successful.</li><li>We have conveniently assumed that the duration of the file download fits within the bucket, so that by the end of the bucket the machines will have downloaded the file.</li><li>The last bucket is bound to have the highest number of concurrent downloads, so most machines will have to wait essentially $(B-1) S$ until they can download the file, where $B$ is the number of time buckets and $S$ is their size.</li></ol><p>We&rsquo;ll work through these details in the next sections.</p><h1 id=allocating-machines-to-time-buckets>Allocating machines to time buckets
<a href=#allocating-machines-to-time-buckets style=float:right;border-bottom:0><svg width="16" height="16" viewBox="0 0 8 8"><path d="M5.88.03c-.18.01-.36.03-.53.09-.27.1-.53.25-.75.47a.5.5.0 10.69.69c.11-.11.24-.17.38-.22.35-.12.78-.07 1.06.22.39.39.39 1.04.0 1.44l-1.5 1.5c-.44.44-.8.48-1.06.47-.26-.01-.41-.13-.41-.13a.5.5.0 10-.5.88s.34.22.84.25 1.2-.16 1.81-.78l1.5-1.5c.78-.78.78-2.04.0-2.81-.28-.28-.61-.45-.97-.53-.18-.04-.38-.04-.56-.03zm-2 2.31c-.5-.02-1.19.15-1.78.75L.6 4.59c-.78.78-.78 2.04.0 2.81.56.56 1.36.72 2.06.47.27-.1.53-.25.75-.47a.5.5.0 10-.69-.69c-.11.11-.24.17-.38.22-.35.12-.78.07-1.06-.22-.39-.39-.39-1.04.0-1.44l1.5-1.5c.4-.4.75-.45 1.03-.44.28.01.47.09.47.09a.5.5.0 10.44-.88s-.34-.2-.84-.22z"/></svg></a></h1><p>In order to achieve this distribution of downloads, we would like to allocate machines to buckets of time such that the number of machines allocated to each bucket is the same as the number of machines that have downloaded the file by that point in time. One way to do this is to sample from a distribution that has this property.</p><p>So, imagine $X$ is a discrete random variable that represents a bucket number, with support over $\{0, 1, \ldots, B-1\}$, where $B$ is the bucket number. We would really like to ensure something similar to $P(X = i+1) = 2 P(X = i)$.</p><p>One way to do so is to ask $P(X = i) = 2^i q$ (which would trivially satisfy the property we&rsquo;d really like to have), then we can solve for $q$, since it must be a probability distribution:</p>$$
\begin{align*}
\sum_{i=0}^{B-1} P(X = i) &= 1 \\
\sum_{i=0}^{B-1} 2^i q &= 1 \\
q \sum_{i=0}^{B-1} 2^i &= 1 \\
q (2^B - 1) &= 1 \\
q &= \frac{1}{2^B - 1}
\end{align*}
$$<p>Which leaves us with $P(X = i) = \frac{2^i}{2^B - 1}$. Now, we have some fancy math which tells us how to distribute machines over buckets, only now we need to sample from this distribution somehow.</p><p><a href=https://en.wikipedia.org/wiki/Inverse_transform_sampling>Inverse transform sampling</a>&nbsp;<img src=/img/logos/wikipedia.svg alt="Site Logo" style=height:1em;vertical-align:middle;margin-right:5px> is a method to sample from a distribution given its cumulative distribution function (CDF). The CDF of this distribution is given by:</p>$$
\begin{align*}
F(x) &= P(X \leq x) \\
&= \sum_{i=0}^{x} P(X = i) \\
&= \sum_{i=0}^{x} \frac{2^i}{2^B - 1} \\
&= \frac{2^{x+1} - 1}{2^B - 1}
\end{align*}
$$<p>and the inverse of that function is:</p>$$
\begin{align*}
F^{-1}(y) &= \text{argmin}_x \left( \frac{2^{x+1} - 1}{2^B - 1} \geq y \right) \\
&= \text{argmin}_x \left( 2^{x+1} - 1 \geq y (2^B - 1) \right) \\
&= \text{argmin}_x \left( 2^{x+1} \geq y (2^B - 1) + 1 \right) \\
&= \text{argmin}_x \left( x+1 \geq \log_2(y (2^B - 1) + 1) \right) \\
&= \lfloor \log_2(y (2^B - 1) + 1) \rfloor
\end{align*}
$$<p>Then, inverse transform sampling tells us to:</p><ol><li>Sample $y$ from a uniform distribution between 0 and 1.</li><li>Compute $x = \lfloor \log_2(y (2^B - 1) + 1) \rfloor$.</li></ol><p>This will give us a sample bucket $x$ from the distribution we want, which looks like this:</p><div id=pdf-plot></div><h1 id=at-last-the-algorithm>At last, the algorithm
<a href=#at-last-the-algorithm style=float:right;border-bottom:0><svg width="16" height="16" viewBox="0 0 8 8"><path d="M5.88.03c-.18.01-.36.03-.53.09-.27.1-.53.25-.75.47a.5.5.0 10.69.69c.11-.11.24-.17.38-.22.35-.12.78-.07 1.06.22.39.39.39 1.04.0 1.44l-1.5 1.5c-.44.44-.8.48-1.06.47-.26-.01-.41-.13-.41-.13a.5.5.0 10-.5.88s.34.22.84.25 1.2-.16 1.81-.78l1.5-1.5c.78-.78.78-2.04.0-2.81-.28-.28-.61-.45-.97-.53-.18-.04-.38-.04-.56-.03zm-2 2.31c-.5-.02-1.19.15-1.78.75L.6 4.59c-.78.78-.78 2.04.0 2.81.56.56 1.36.72 2.06.47.27-.1.53-.25.75-.47a.5.5.0 10-.69-.69c-.11.11-.24.17-.38.22-.35.12-.78.07-1.06-.22-.39-.39-.39-1.04.0-1.44l1.5-1.5c.4-.4.75-.45 1.03-.44.28.01.47.09.47.09a.5.5.0 10.44-.88s-.34-.2-.84-.22z"/></svg></a></h1><p>This algorithm has two sides to it: the producer of the file, and the consumer of the file. On the producer&rsquo;s end, we will keep everything as-is, but we&rsquo;ll add some metadata to the file:</p><ol><li>The file creation time $T_{\text{current}}$.</li><li>The expected time of creation for the <em>next</em> file $T_{\text{next}}$, if it&rsquo;s known.</li></ol><p>On the consumer&rsquo;s end, the key thing is this: we might poll for the file multiple times before a new file is created, so we <em>need</em> to ensure that whatever bucket we&rsquo;re in, it&rsquo;s consistent over time.</p><p>So, we know that:</p><ol><li>We have $N$ machines that need to download the data.</li><li>They need to be distributed over $B = \lceil \log_2(N) \rceil$ buckets.</li><li>We need to sample from the distribution we derived above to determine which bucket we&rsquo;re in.</li><li>We need to ensure that the bucket we&rsquo;re in is consistent over time every time we poll until the next file is created.</li><li>We have $\delta = T_{\text{next}} - T_{\text{current}}$ until the next file is created.</li></ol><p>The missing piece here is determining the size $S$ of the buckets. This can be done in any number of ways, such as $S = \frac{\frac{1}{2} \delta}{B}$, or via some sort of estimate of the download time. The important part is that $T_{\text{current}} + S B < T_{\text{next}}$, so all machines are done downloading before the next file is available.</p><p>Now that we have all this, we need to determine which bucket we&rsquo;re in. The problem is that if we follow the inverse transform sampling method, we will get a different bucket every time we poll for a new file. This is not what we want, as we want the machine to be assigned the same bucket every single time it checks, at least until a new file is available to download.</p><p>One easy way to do this is to use hashing. We can hash the file creation time and the machine name to get a number that&rsquo;s roughly uniformly distributed (as in $r = \text{hash}(\text{fileCreationTime}, \text{machineName})$), and then use that number to determine the bucket by setting $B = F^{-1}(r)$. This way, the machine will always be assigned the same bucket until a new file is created.</p><p>The rest of the algorithm is straightforward. Every time we check for a new file, we do the following:</p><ol><li>If there is no new file, we do nothing.</li><li>Otherwise, generate $r$, and sample the current bucket $B$. We proceed to download the file only if $T_{\text{next}} \geq \text{now} \geq T_{\text{current}} + S B$</li></ol><h1 id=what-was-thrown-under-the-rug>What was thrown under the rug
<a href=#what-was-thrown-under-the-rug style=float:right;border-bottom:0><svg width="16" height="16" viewBox="0 0 8 8"><path d="M5.88.03c-.18.01-.36.03-.53.09-.27.1-.53.25-.75.47a.5.5.0 10.69.69c.11-.11.24-.17.38-.22.35-.12.78-.07 1.06.22.39.39.39 1.04.0 1.44l-1.5 1.5c-.44.44-.8.48-1.06.47-.26-.01-.41-.13-.41-.13a.5.5.0 10-.5.88s.34.22.84.25 1.2-.16 1.81-.78l1.5-1.5c.78-.78.78-2.04.0-2.81-.28-.28-.61-.45-.97-.53-.18-.04-.38-.04-.56-.03zm-2 2.31c-.5-.02-1.19.15-1.78.75L.6 4.59c-.78.78-.78 2.04.0 2.81.56.56 1.36.72 2.06.47.27-.1.53-.25.75-.47a.5.5.0 10-.69-.69c-.11.11-.24.17-.38.22-.35.12-.78.07-1.06-.22-.39-.39-.39-1.04.0-1.44l1.5-1.5c.4-.4.75-.45 1.03-.44.28.01.47.09.47.09a.5.5.0 10.44-.88s-.34-.2-.84-.22z"/></svg></a></h1><p>As per above, we assumed that all file downloads are successful, and that the download time fits within the bucket. In my particular production scenario, I had $1 \text{m}$ buckets with $\delta = 15 \text{m}$, and downloads would certainly fit within that bucket, so I didn&rsquo;t have to worry about it. If you have a different scenario, you might need to adjust the bucket size accordingly.</p><p>The other issue is that we assumed all file downloads are successful. This is a bit of a problem, as we don&rsquo;t have a way to ensure that all file downloads are successful. Due to particular details of the production system I was working with, this didn&rsquo;t actually matter because the probability of a file download failing was in practice very low. This can, however, be somewhat accounted for by modifying the distribution to account for the probability of a file download failing, and then sampling from that renewed distribution.</p><h1 id=conclusion>Conclusion
<a href=#conclusion style=float:right;border-bottom:0><svg width="16" height="16" viewBox="0 0 8 8"><path d="M5.88.03c-.18.01-.36.03-.53.09-.27.1-.53.25-.75.47a.5.5.0 10.69.69c.11-.11.24-.17.38-.22.35-.12.78-.07 1.06.22.39.39.39 1.04.0 1.44l-1.5 1.5c-.44.44-.8.48-1.06.47-.26-.01-.41-.13-.41-.13a.5.5.0 10-.5.88s.34.22.84.25 1.2-.16 1.81-.78l1.5-1.5c.78-.78.78-2.04.0-2.81-.28-.28-.61-.45-.97-.53-.18-.04-.38-.04-.56-.03zm-2 2.31c-.5-.02-1.19.15-1.78.75L.6 4.59c-.78.78-.78 2.04.0 2.81.56.56 1.36.72 2.06.47.27-.1.53-.25.75-.47a.5.5.0 10-.69-.69c-.11.11-.24.17-.38.22-.35.12-.78.07-1.06-.22-.39-.39-.39-1.04.0-1.44l1.5-1.5c.4-.4.75-.45 1.03-.44.28.01.47.09.47.09a.5.5.0 10.44-.88s-.34-.2-.84-.22z"/></svg></a></h1><p>This algorithm is a simple way to ensure that the load on a producer of a file is distributed over time. It&rsquo;s obviously not perfect and has a number of underlying assumptions, but it does have the benefit that it can be implemented in like 10 lines of code and doesn&rsquo;t require much infrastructure. It&rsquo;s a neat trick to have in your back pocket if you ever find yourself in a situation where you need to distribute load over time, or more likely as inspiration if you ever find yourself in a similar situation.</p><p>I have run this algorithm in production and it has worked without any issues over the course of what is now several years (and in fact, the load balancing it introduced eliminated several ways in which the system used to fail).</p></div><div class=content data-pagefind-ignore=all><br><div class=boxed><h3>Related Articles</h3><ol class=unlist><li><span class="subtitle is-6">(March 31, 2024)</span> <a href=/post/sketching-lru/>Cheap approximate LRU eviction with sketching</a></li><li><span class="subtitle is-6">(May 2, 2018)</span> <a href=/post/understanding-modulo-bias/>Understanding Modulo Bias</a></li></ol></div></div><button id=load-comments-button class="button is-primary is-active is-large is-fullwidth" style=display:none>Load Comments</button>
<script src=/js/comments.min.39e33edd67905b2b36477104baf1ec48ae8d66edab407617757a4adf0f6b75a0.js integrity="sha256-OeM+3WeQWys2R3EEuvHsSK6NZu2rQHYXdXpK3w9rdaA=" crossorigin=anonymous async defer></script></div></div></article></section><div class=top-button><a href=https://julian.bayardo.info/post/pacemaker/#><svg width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-arrow-up"><line x1="12" y1="19" x2="12" y2="5"/><polyline points="5 12 12 5 19 12"/></svg></a></div><script src=/js/toc-highlight.min.e7ab37b0afc209abb07ffa8102b73d478c9b74c36c80593bdccc9793fc60adb5.js integrity="sha256-56s3sK/CCauwf/qBArc9R4ybdMNsgFk73MyXk/xgrbU=" crossorigin=anonymous async defer></script><footer class=section style=padding-top:0><div class=container><hr><p style=float:left>&copy; Julian Bayardo Spadafora 2015-2024 | <a href=/privacy-policy/>Privacy Policy</a> | <a href=/terms-of-service/>Terms of Service</a></p><div style=float:right><a href=https://julian.bayardo.info/post/pacemaker/#>Back to Top</a></div></div></footer><script>window.MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]],displayMath:[["$$","$$"],["[","]"]],processEscapes:!0,processEnvironments:!0,autoload:{color:[],colorV2:["color"]},packages:{"[+]":["noerrors"]}},options:{skipHtmlTags:["script","noscript","style","textarea","pre","code"],ignoreHtmlClass:"tex2jax_ignore",processHtmlClass:"tex2jax_process",renderActions:{find_script_mathtex:[10,function(e){for(const t of document.querySelectorAll('script[type^="math/tex"]')){const o=!!t.type.match(/; *mode=display/),n=new e.options.MathItem(t.textContent,e.inputJax[0],o),s=document.createTextNode("");t.parentNode.replaceChild(s,t),n.start={node:s,delim:"",n:0},n.end={node:s,delim:"",n:0},e.math.push(n)}},""]}},loader:{load:["input/asciimath","[tex]/noerrors"]}}</script><script type=text/javascript id=MathJax-script async defer src=/js/vendor/mathjax/tex-mml-chtml.js></script><script type=text/javascript async defer src=/post/pacemaker/plots.js></script></body></html>